---
layout: post
title: Azure OpenAI GPT-Modelle - Vollst√§ndiger √úberblick ‚Äì leicht verst√§ndlich
tags: [CSP, Azure, GPT, Azure AI Foundry, Modelle]
---

Es gibt inzwischen so viele KI-Modelle √ºber Azure OpenAI ‚Äì da den √úberblick zu behalten, kann ganz sch√∂n herausfordernd sein. Aber keine Sorge, ich erkl√§re dir in diesem Beitrag alles Schritt f√ºr Schritt, verst√§ndlich und anschaulich. Von GPT-3.5 √ºber o-Serien, GPT-4-Reihe bis hin zu GPT-5, Audio- und Bildmodellen ‚Äì und sogar dem Modellrouter. Au√üerdem zeige ich dir, welche Modelle f√ºr welche Use Cases am besten geeignet sind.!<br>

Wenn du in Azure AI Foundry KI-Modelle ausrollen m√∂chtest, gibt es ein paar Dinge, die wichtig sind ‚Äì auch wenn du dich bei Foundry-Modellen wie GPT nicht um die eigentliche Infrastruktur k√ºmmern musst. Microsoft stellt diese Modelle als Managed Service bereit, das hei√üt: Skalierung, GPUs und Hochverf√ºgbarkeit laufen automatisch im Hintergrund. Deine Rolle verschiebt sich dadurch: Statt Compute-Kapazit√§ten zu planen, konzentrierst du dich auf die richtige Modellwahl, Kostenkontrolle und die Einbettung in deine Anwendungen. Achte darauf, von Anfang an sauber zwischen Test- und Produktionsumgebung zu trennen, um Transparenz und Kosten im Griff zu behalten. Sicherheit spielt ebenfalls eine zentrale Rolle: Richte Rollen und Berechtigungen sorgf√§ltig ein und denke an Datenschutz und Verschl√ºsselung, vor allem wenn sensible Daten verarbeitet werden. Ein weiterer Erfolgsfaktor ist Monitoring: Auch wenn du das Modell nicht selbst betreibst, solltest du Protokollierung, Auslastung und Antwortqualit√§t im Blick behalten ‚Äì so erkennst du fr√ºhzeitig, ob dein Setup angepasst werden muss. Und schlie√ülich ist es sinnvoll, die Kostenmodelle gut zu verstehen, da die Abrechnung meist nutzungsbasiert √ºber Tokens erfolgt. Kurz gesagt: Bei Foundry-Modellen musst du dich nicht um Hardware oder Compute k√ºmmern, aber daf√ºr umso mehr um Governance, Sicherheit, Monitoring und Kostenmanagement.<br><br>

Die gesamte Infrastruktur ‚Äì GPUs, Skalierung, Updates, Verf√ºgbarkeit ‚Äì liegt bei Microsoft. F√ºr dich bleibt die Perspektive eine reine API-Nutzung:

<li>Du rufst das Modell √ºber einen Endpunkt auf.</li>
<li>Du zahlst nach Nutzungseinheiten (z. B. Tokens).</li>
<li>Du musst keine VM starten, keine GPU ausw√§hlen und auch kein Autoscaling konfigurieren.</li>
<li>Du achtest eher auf Themen wie Kostenkontrolle, Rate Limits, Zugriffssicherheit, Daten- und Prompt-Management sowie Governance.</li><br>

üëâ Nur wenn du eigene Modelle trainierst oder hostest, kommst du wieder in die Welt der Rechenressourcen zur√ºck.<br><br>

## Welches Modell f√ºr welchen Use Case?

Neben den bekannten OpenAI-Modellen wie GPT-4 oder GPT-5 gewinnen zunehmend auch andere Sprachmodelle an Bedeutung, die man als ‚Äûexotischer‚Äú bezeichnen k√∂nnte. Dazu z√§hlen insbesondere **Grok** von xAI, **DeepSeek** aus China, das europ√§ische **Mistral** sowie **Llama** von Meta. Sie unterscheiden sich teils deutlich in Herkunft, Architektur, Verf√ºgbarkeit und Anwendungsbereichen ‚Äì und genau das macht sie f√ºr Unternehmen spannend, die Alternativen zu klassischen OpenAI-Modellen suchen.

**Grok**, entwickelt von Elon Musks Unternehmen xAI, ist vor allem f√ºr seine starke Integration von Echtzeitdaten bekannt. W√§hrend klassische GPT-Modelle auf statischen Trainingsdaten basieren und Aktualit√§t durch nachtr√§gliche Tools wie RAG oder Plugins erhalten m√ºssen, kann Grok direkt auf aktuelle Informationen zugreifen. Das macht es besonders interessant f√ºr Anwendungsf√§lle wie Finanzanalysen, Social-Media-Monitoring oder Nachrichtendienste, die dynamische Inhalte in Echtzeit ben√∂tigen. Allerdings ist der Zugang noch eingeschr√§nkt, da Grok bislang prim√§r √ºber die xAI-Plattform genutzt werden kann. Hinzu kommen datenschutzrechtliche Bedenken, da in der Vergangenheit √ºber √∂ffentlich sichtbare Nutzerinteraktionen diskutiert wurde. F√ºr Projekte, die h√∂chste Compliance-Anforderungen haben, ist hier also Vorsicht geboten.

**DeepSeek** stammt aus China und hat sich schnell einen Namen gemacht, weil es leistungsstarke Sprachmodelle zu einem Bruchteil der Kosten etablierter Anbieter anbietet. Besonders stark ist DeepSeek in logischen Schlussfolgerungen, strukturiertem Denken und Planungsaufgaben ‚Äì F√§higkeiten, die beispielsweise bei Finanzplanung, Projektorganisation oder im Bildungsbereich sehr wertvoll sind. Damit positioniert sich DeepSeek als kosteneffiziente Alternative zu westlichen Modellen. Allerdings ist die internationale Verf√ºgbarkeit noch eingeschr√§nkt, und die Modelle sind teils auf bestimmte Hardware optimiert, was die Flexibilit√§t reduziert.

Das franz√∂sische **Mistral** verfolgt einen komplett anderen Ansatz: Als europ√§isches Open-Source-Sprachmodell setzt es auf Transparenz und Anpassbarkeit. Der gro√üe Vorteil liegt darin, dass Entwickler das Modell nach eigenen Bed√ºrfnissen trainieren, erweitern oder in bestehende Systeme integrieren k√∂nnen, ohne an einen einzelnen Anbieter gebunden zu sein. Das macht Mistral besonders interessant f√ºr Forschung, √∂ffentliche Institutionen oder Unternehmen mit hohen Anforderungen an Datenkontrolle und Anpassbarkeit. Die Kehrseite ist, dass Open-Source-Modelle wie Mistral meist einen h√∂heren Ressourcenbedarf haben und f√ºr produktive Szenarien entsprechend starke Infrastruktur erfordern. Zudem h√§ngt die Weiterentwicklung stark von der Community ab ‚Äì was einerseits Innovation f√∂rdert, andererseits aber weniger verl√§ssliche Roadmaps mit sich bringt.

**Llama**, entwickelt von Meta, ist mittlerweile die bekannteste Open-Source-Alternative zu GPT. Llama-Modelle sind in verschiedenen Gr√∂√üen verf√ºgbar, von leichteren Varianten f√ºr experimentelle Anwendungen bis hin zu sehr gro√üen Modellen f√ºr anspruchsvolle Workloads. Besonders hervorzuheben ist die zunehmende Multimodalit√§t: Llama kann nicht nur Text verarbeiten, sondern auch Bilder und andere Eingaben. Dadurch wird es zu einer flexiblen Plattform f√ºr unterschiedlichste Anwendungen ‚Äì von Chatbots √ºber Dokumentenanalyse bis hin zu kreativen Projekten, die Text und visuelle Elemente verbinden. F√ºr Entwickler und Unternehmen, die Open-Source-Modelle produktiv einsetzen m√∂chten, bietet Llama dank einer gro√üen Community und starker Unterst√ºtzung durch Meta eine vergleichsweise stabile Grundlage.

Zusammengefasst lohnt sich auch ein Blick auf diese ‚Äûexotischeren‚Äú Modelle vor allem dann, wenn man nach Alternativen zu klassischen Azure OpenAI-Modellen sucht. Grok √ºberzeugt mit Echtzeitf√§higkeit, DeepSeek mit Kosteneffizienz und starkem strukturiertem Denken, Mistral mit Offenheit und Anpassbarkeit, und Llama mit Multimodalit√§t und einer starken Open-Source-Community. Wichtig ist dabei immer, die jeweiligen St√§rken mit den eigenen Anforderungen abzugleichen: Wer Stabilit√§t und garantierte Roadmaps braucht, ist bei Meta oder OpenAI besser aufgehoben, w√§hrend Mistral und Llama Flexibilit√§t und Kontrolle erm√∂glichen. DeepSeek kann ein Kostenvorteil sein, bringt aber Abh√§ngigkeiten von regionalen Infrastrukturen mit. Grok wiederum ist hochinnovativ, aber aktuell nur eingeschr√§nkt nutzbar.

Die gro√üe St√§rke der Azure OpenAI Plattform liegt darin, dass du dir genau das Modell aussuchen kannst, das am besten zu deinem Projekt passt. Damit du nicht nur Tabellenwerte siehst, sondern auch verstehst, wof√ºr sich die einzelnen Varianten lohnen, habe ich die wichtigsten Anwendungsf√§lle hier ausf√ºhrlicher beschrieben:

in der folgenden √úbersicht, gehe ich jedoch nur noch auf die g√§ngigsten GPT-Modelle ein.

| Use Case |	Modellgruppe |	Beschreibung & Vorteile |
| Standard-Textverarbeitung |	GPT-3.5, GPT-4 |	Ideal, wenn du klassische Chatbots, FAQ-Antworten oder Texterstellung brauchst. Diese Modelle sind zuverl√§ssig, laufen stabil und sind im Vergleich zu den neuesten Generationen g√ºnstiger. |
| Komplexe Analysen & Logik |	o-Series, GPT-4	| Wenn es um tiefere Analysen, datenbasierte Auswertungen oder komplexes Probleml√∂sen geht, bist du hier richtig. Diese Modelle k√∂nnen lange Kontexte verarbeiten und sind stark im ‚Äûlogischen Denken‚Äú. |
| Multimodale Aufgaben (Text & Bild) |	GPT-4o, GPT-4 Turbo |	Du willst Bilder beschreiben lassen oder Text mit Bildinhalten kombinieren? Diese Modelle verstehen und erzeugen multimodale Inhalte und sind perfekt f√ºr Szenarien wie Dokumentanalyse mit Bildanteilen oder kreative Content-Erstellung. |
| Realtime Sprachdialoge |	GPT-4o-realtime-preview |	Stelle dir vor, du sprichst mit einer KI wie mit einem echten Menschen. Diese Variante erm√∂glicht latenzarme Sprach-zu-Sprach-Kommunikation ‚Äì perfekt f√ºr Assistenten, Callcenter oder interaktive Sprachtools. |
| Transkription / TTS |	GPT-4o Audio-Modelle |	Wenn du Meetings transkribieren, Podcasts zusammenfassen oder Sprache in Text (und umgekehrt) verwandeln willst, bist du hier richtig. Die Audio-Modelle sind optimiert f√ºr Spracheingaben und -ausgaben. |
| Automatisiertes Modell-Routing |	model-router |	Du willst dich gar nicht mit Modellwahl besch√§ftigen? Der Router nimmt dir diese Entscheidung ab und leitet deine Anfrage automatisch an das am besten passende Modell weiter. Ideal f√ºr Systeme, die flexibel bleiben m√ºssen. |
| Tool-getriebene Antworten (Code etc.) |	computer-use-preview |	Dieses Modell ist f√ºr Szenarien gedacht, in denen die KI gezielt Tools steuert oder externe Systeme anbindet. Ein Beispiel: Code schreiben und gleichzeitig gleich testen lassen. |
| High-End Reasoning & gro√üe Kontexte |	GPT-5 (Standard) |	Hier bekommst du die volle Power: riesige Kontextfenster, logisches Denken, multimodale Verarbeitung. Geeignet f√ºr wissenschaftliche Analysen, komplexe Unternehmensprozesse oder Forschung. |
| Schnelle, ressourcenschonende KI |	GPT-5 mini / nano |	Nicht jede Aufgabe braucht die ‚Äûgro√üe Kanone‚Äú. Mini- und Nano-Modelle sind optimiert auf Geschwindigkeit und Kosten ‚Äì ideal f√ºr einfache Chatbots, Microservices oder Apps mit vielen gleichzeitigen Usern. |
| Chat-Agent mit Kontextmanagement  |	GPT-5 chat | Wenn du l√§ngere Gespr√§che f√ºhrst, in denen der Kontext nicht verloren gehen darf, spielt dieses Modell seine St√§rke aus. Ideal f√ºr Support-Chatbots oder pers√∂nliche Assistenten. |
| Lokale Nutzung / Offline |	gpt-oss-20B / 120B (Open-Weight) |	F√ºr alle, die ihre Daten nicht in die Cloud geben k√∂nnen oder wollen: Die Open-Weight-Modelle laufen lokal und bieten dir maximale Kontrolle ‚Äì mit dem Nachteil, dass du selbst die Infrastruktur stemmen musst. |<br>

üí° Tipp: Wenn du viele Anfragen gleichzeitig stellen m√∂chtest, ist GPT-5 Nano oder Mini aufgrund ihrer hohen TPM-Werte besonders geeignet. F√ºr kreative Aufgaben wie Storytelling oder Content Creation ist GPT-4.5 optimal, w√§hrend f√ºr komplexe logische Aufgaben GPT-5 Standard die beste Wahl ist.<br><br>

## Token-Kapazit√§t und Abrechnungsmodelle: TPM & PTU

Neben der Wahl des passenden Modells ist auch die Art der Bereitstellung entscheidend.
<li>TPM (Tokens per Minute): Standard-Limitierung im Pay-as-you-go-Modell. Sie legt fest, wie viele Tokens pro Minute verarbeitet werden d√ºrfen. Gut geeignet f√ºr Prototypen oder kleinere Workloads.</li><br>
<li>PTU (Provisioned Throughput Unit): Reservierte Kapazit√§t mit garantierten Durchsatzwerten. PTUs stellen sicher, dass die Latenz auch bei hoher Auslastung stabil bleibt. Besonders interessant f√ºr produktive Szenarien oder wenn Lastspitzen vorhersehbar sind.</li>

| Merkmal |	TPM (Tokens per Minute) |	PTU (Provisioned Throughput Unit) |
| Abrechnung |	Pay-as-you-go (pro Token) |	Fixpreis pro reservierter Einheit |
| Kapazit√§t |	Geteilt mit anderen Kunden |	Dediziert, garantiert |
| Leistung |	variabel, abh√§ngig von Auslastung |	stabile Latenz, vorhersagbar |
| Empfohlen f√ºr |	Tests, Prototypen, geringe Last |	Produktion, hohe Auslastung, Business-kritisch |<br>

## Erweiterte Empfehlungen: Modelle & Use-Cases PTU's vs. TPM's

| Use Case |	Empfohlenes Modell |	Warum dieses Modell? |	Empfohlene Kapazit√§t |
| Kundensupport / Chatbot (24/7) |	gpt-5-chat oder gpt-4.1-mini |	Optimiert f√ºr Dialoge, kosteneffizient, gute Sprachqualit√§t |	Start: PAYG mit TPM. Bei >50k Usern oder Peaks ‚Üí PTU f√ºr Stabilit√§t |
| Dokumentenanalyse / Long Context Q&A |	gpt-4.1 oder gpt-5 |	Sehr lange Kontexte (bis 1 Mio Tokens), gutes Reasoning	| PTU empfohlen (lange Prompts verursachen hohes Token-Volumen, TPM k√∂nnte schnell limitieren) |
| Code-Generierung & Dev-Assistenz |	o4-mini, Codex-mini, gpt-5-nano |	Kurze Latenz, gut im Code-Reasoning, g√ºnstig |	TPM reicht oft aus; PTU lohnt sich nur bei konstant hohen Builds/CI-Integrationen |
| Wissens-Chatbot f√ºr Unternehmen	| Phi-4 oder gpt-5-chat	Phi-4: | sehr effizientes Reasoning, g√ºnstig; GPT-5 f√ºr Multimodalit√§t |	TPM f√ºr Prototypen; PTU bei >10 gleichzeitigen Sessions im Unternehmenskontext |
| Multimodale Szenarien (Bild+Text)	| gpt-4-turbo-vision, gpt-5 |	Kombination von Text & Bildern, auch f√ºr Content-Analyse |	TPM reicht f√ºr Tests; PTU sinnvoll bei z. B. automatisierter Bildpipeline |
| High-Volume API (z. B. Suche, RAG) |	gpt-4.1-nano, o3 |	Sehr schnelle & g√ºnstige Antworten, ideal f√ºr Einbettung in RAG-Pipelines |	PTU empfohlen, da konstante, vorhersehbare Antwortzeit wichtig |
| Forschung & volle Kontrolle |	gpt-oss-120b oder gpt-oss-20b	| Open-Weight, volle Kontrolle & Reproduzierbarkeit |	PTU fast immer sinnvoll ‚Äì PAYG hat zu viele Limits f√ºr Forschung |
| Copilot-√§hnliche Integrationen |	gpt-5, gpt-4.5 preview |	Hohe Qualit√§t, gute Integration in Tools & Workflows |	TPM f√ºr MVP; PTU bei produktivem Rollout mit hoher Nutzlast |<br><br>

## Leitfaden: TPM oder PTU?

**TPM (Tokens per Minute)**
üëâ Ideal f√ºr kleine bis mittlere Projekte, Pilotphasen, Proof-of-Concepts.
üëâ Flexibel, keine Bindung, du zahlst nur pro Token.

**PTU (Provisioned Throughput Unit)**
üëâ Ideal bei planbarer Last (z. B. t√§gliche Verarbeitung von tausenden Dokumenten oder Millionen Chatnachrichten).
üëâ Garantierte Latenz, weniger Schwankungen, skalierbar.
üëâ Kosteneffizient ab einem bestimmten Nutzungsvolumen (Break-Even oft schon bei wenigen Mio Tokens pro Tag).<br><br>


## Regionale Verf√ºgbarkeit der Modelle
<li><b>GPT-5 Modelle</b> (Standard, mini, nano, chat) sind aktuell in <b>East US 2</b> und <b>Sweden Central</b> verf√ºgbar. F√ºr GPT-5 ist eine Registrierung n√∂tig, die kleineren Varianten hingegen nicht</li><br>
<li>Die √§lteren Modelle <b>(GPT-4, o-Serien, GPT-4o etc.)</b> sind breit √ºber viele Regionen verf√ºgbar ‚Äì beispielweise auch in <b>Sweden Central, Germany West, East US, West US</b> und weiteren</li><br>

üí° Tipp: <b>Sora</b> ist aktuell nur eingeschr√§nkt verf√ºgbar. Wenn du also Videos einsetzen willst, solltest du vorher pr√ºfen, ob das Modell in deiner Region freigeschaltet ist.<br><br>
**Wichtig**: Nicht alle Modelle sind √ºber alle Bezugskan√§le verf√ºgbar. Die Llama Modelle zum Beispiel gibt es nur, wenn  

## Azure Speech Service vs. gpt-4o Realtime Preview
Der Azure Speech Service und gpt-4o Realtime Preview erf√ºllen unterschiedliche Rollen: Speech Service ist ein spezialisierter Sprachdienst f√ºr pr√§zise Transkription (ASR), nat√ºrlich klingende Text-to-Speech-Stimmen, √úbersetzungen und Speaker-Features wie Diarisierung. Er eignet sich vor allem, wenn es um saubere Transkripte, Anpassungen mit Fachvokabular, Custom Voices oder sogar On-Prem-Deployments mit strengen Datenschutzanforderungen geht und wird nach Audio-Stunden oder erzeugten Zeichen abgerechnet. gpt-4o Realtime hingegen ist ein multimodales Sprachmodell, das Audio direkt versteht und in Echtzeit generative Antworten als Text oder Sprache zur√ºckgibt ‚Äì also perfekt f√ºr interaktive Dialog-Systeme, Assistenten oder Voice Agents, bei denen es auf nat√ºrliche Konversationen ankommt. Technisch setzt es auf WebRTC/WebSockets f√ºr niedrige Latenz, hat in der Preview bestimmte Limits (z. B. ca. 100.000 Tokens pro Minute und 1.000 Requests pro Minute) und wird tokenbasiert abgerechnet. W√§hrend Speech Service f√ºr h√∂chste Transkriptionsqualit√§t und anpassbare Stimmen die bessere Wahl ist, punktet gpt-4o Realtime bei kontextreichen Gespr√§chen und generativer Intelligenz. In vielen Szenarien erg√§nzt sich beides: Speech Service liefert die robuste Sprachbasis, gpt-4o Realtime sorgt f√ºr die intelligente, konversationsf√§hige Ebene.<br><br>
<b>Zu diesem Thema werden ich einen eigenen Blogbeitrag verfassen. Sei bespannt, welche Insides ich hier f√ºr dich hier habe.</b>

## Fazit

Mit Azure AI Foundry erh√§ltst du Zugang zu einer breiten Palette von Modellen ‚Äì von klassischen Chatbots √ºber Bild-/Audio-KI bis hin zu leistungsf√§higen GPT-5-Varianten. √úberlege dir einfach:
<li><b>1.) Was ist dein Use Case?</b> (Text, Bild, Audio, Chat-Agent, Code etc.)</li>
<li><b>2.) Wie viel Leistung & Token-Kapazit√§t brauchst du?</b></li>
<li><b>3.) Wo wird das Modell gehostet?</b> (Regionale Verf√ºgbarkeit checken)</li>
<li><b>4.) Welche TPM- und Kostenbedingungen sind akzeptabel?</b></li><br>
Dann w√§hlst du das Modell, das am besten passt ‚Äì und nutzt es effizient, effizient und kostenbewusst.
